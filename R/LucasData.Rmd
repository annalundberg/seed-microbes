---
title: "When to Assemble Reads"
author: "Lucas Nebert""
date: "`r date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Documents/Lucas")

library(ape); packageVersion("ape")
library(phyloseq); packageVersion("phyloseq")
library(ggplot2); packageVersion("ggplot2")
library(dada2); packageVersion('dada2')
library(ShortRead)
library(dplyr)
library(rPython)

set.seed(777)
```

## problem
The issue is with merging two sequencing runs —— Lucas has two sequencing runs with most samples sequenced in both runs. When processing with DADA2, he ends up with  only about 5% shored taxa between samples sequenced in both runs. (It really should be pretty close to 100%!) So the issue is: why isn't it picking the same RSVs from the same samples? How do we merge the sequencing runs? 

With USEARCH in QIIME for otu picking samples included in both runs are nearly identical. With DADA2, they're super different. So it's something about the application of DADA2.

## Separate the 16S from the ITS


```{r, eval=FALSE}
system("cd data/run1; for f in *R1_run1.fastq; do python parse_microbes.py $f; done")
system("cd data/run2; for f in *R1_run2.fastq; do python parse_microbes.py $f; done")
```

Note, Run1 contains both 16S and ITS sequences, while run2 only contains 16S sequences

## file parsing

Let's parse out the files for both runs, and restrict ourselves to just the 16S to begin with:

```{r, echo=FALSE}
# Run1
path1 <- './data/run1/16S'
filtpathF1 <- file.path(path1, 'filteredF1') ## filtered files go into the filtered/ subdirectory
filtpathR1 <- file.path(path1, 'filteredR1') ## filtered files go into the filtered/ subdirectory

dir.create(filtpathF1)
dir.create(filtpathR1)
fns1 <- list.files(path1)
fastqs1 <- fns1[grepl('.fastq$', fns1)] # CHANGE if different file extensions

# Run2
path2 <- './data/run2/16S'
filtpathF2 <- file.path(path2, 'filteredF2') ## filtered files go into the filtered/ subdirectory
filtpathR2 <- file.path(path2, 'filteredR2') ## filtered files go into the filtered/ subdirectory

dir.create(filtpathF2)
dir.create(filtpathR2)
fns2 <- list.files(path2)
fastqs2 <- fns2[grepl('.fastq$', fns2)] # CHANGE if different file extensions
```

## fully specify the path for the forward and reverse reads
```{r, echo=FALSE}
# Run1
fnFs1 <- file.path(path1, fastqs1[grepl("_R1", fastqs1)] )# Just the forward read files
fnRs1 <- file.path(path1, fastqs1[grepl("_R2", fastqs1)] )# Just the reverse read files

sample.names.F1 <- sapply(strsplit(fastqs1[grepl("_R1", fastqs1)], "_"), `[`, 2)
sample.names.R1 <- sapply(strsplit(fastqs1[grepl("_R2", fastqs1)], "_"), `[`, 2)

sample.names.F1 <- paste(sample.names.F1,"F", sep="_")
sample.names.R1 <- paste(sample.names.R1,"R", sep="_")

# Run2
fnFs2 <- file.path(path2, fastqs2[grepl("_R1", fastqs2)] )# Just the forward read files
fnRs2 <- file.path(path2, fastqs2[grepl("_R2", fastqs2)] )# Just the reverse read files

sample.names.F2 <- sapply(strsplit(fastqs2[grepl("_R1", fastqs2)], "_"), `[`, 2)
sample.names.R2 <- sapply(strsplit(fastqs2[grepl("_R2", fastqs2)], "_"), `[`, 2)

sample.names.F2 <- paste(sample.names.F2,"F", sep="_")
sample.names.R2 <- paste(sample.names.R2,"R", sep="_")
```

## examine quality profiles (random sample)
```{r}
# from Run2, sample 17
plotQualityProfile(fnFs2[[9]])
plotQualityProfile(fnRs2[[9]])

# From Run1, sample 17
plotQualityProfile(fnFs1[[1]])
plotQualityProfile(fnRs1[[1]])
```

Looking at a handful of samples, it looks basically as expected, with forward reads generally a lot better than reverse reads, but the forward reads look pretty shitty compared to what I'm used to. I want to focus in on Sample 17 from both runs, to try to get some idea of what's going on here. 

# Make directory and filenames for the filtered fastqs
```{r}
# Run1 
filtFs1 <- file.path(filtpathF1, paste0(sample.names.F1, "_filt.fastq.gz"))
filtRs1 <- file.path(filtpathR1, paste0(sample.names.R1, "_filt.fastq.gz"))

# Run2
filtFs2 <- file.path(filtpathF2, paste0(sample.names.F2, "_filt.fastq.gz"))
filtRs2 <- file.path(filtpathR2, paste0(sample.names.R2, "_filt.fastq.gz"))
```

## filtering step
```{r, eval=F}
# Run1
for(i in seq_along(fnFs1)) {
  fastqPairedFilter(c(fnFs[i], fnRs[i]), c(filtFs[i], filtRs[i]),
                    trimLeft=10,
                    truncLen=c(200,180), 
                    maxN=0, maxEE=c(3,3), truncQ=2, rm.phix=TRUE,
                    compress=TRUE, verbose=TRUE)
}

# Run2
for(i in seq_along(fnFs2)) {
  fastqPairedFilter(c(fnFs[i], fnRs[i]), c(filtFs[i], filtRs[i]),
                    trimLeft=10,
                    truncLen=c(200,180), 
                    maxN=0, maxEE=c(3,3), truncQ=2, rm.phix=TRUE,
                    compress=TRUE, verbose=TRUE)
}
```

I've raised the maxEE (max expected error) at 3 for FOR and REV reads. Given the early drop off in quality, we probably want to let more errors through in pairing. We also moved the truncation in to 200 and 180, to get rid of the dirty regions, which improved the number of sequences retained. 

## Single sample testing: #17

Okay, very few sequences are making it through quality filtering for run2 as compared to run1. I'm going to go back to my example sample (17) and just pull out that one for quality filtering and downstream analysis. 

```{r}
## TRUNCTATION: Lax
# Filter just sample 17 from Run1
fastqPairedFilter(c(fnFs1[1], fnRs1[1]), c(filtFs1[1], filtRs1[1]),
                    trimLeft=10,
                    truncLen=c(210,190), 
                    maxN=0, maxEE=c(3,3), truncQ=2, rm.phix=TRUE,
                    compress=TRUE, verbose=TRUE)

# Filter just sample 17 from Run2
fastqPairedFilter(c(fnFs2[9], fnRs2[9]), c(filtFs2[9], filtRs2[9]),
                    trimLeft=10,
                    truncLen=c(210,190), 
                    maxN=0, maxEE=c(3,3), truncQ=2, rm.phix=TRUE,
                    compress=TRUE, verbose=TRUE)

## TRUNCATION: Strict
# Filter just sample 17 from Run1
fastqPairedFilter(c(fnFs1[1], fnRs1[1]), c(filtFs1[1], filtRs1[1]),
                    trimLeft=10,
                    truncLen=c(190,170), 
                    maxN=0, maxEE=c(3,3), truncQ=2, rm.phix=TRUE,
                    compress=TRUE, verbose=TRUE)

# Filter just sample 17 from Run2
fastqPairedFilter(c(fnFs2[9], fnRs2[9]), c(filtFs2[9], filtRs2[9]),
                    trimLeft=10,
                    truncLen=c(190,170), 
                    maxN=0, maxEE=c(3,3), truncQ=2, rm.phix=TRUE,
                    compress=TRUE, verbose=TRUE)
```

So run2 lets many fewer sequences through, which makes sense given the quality plots. Aso, tructation makes a huge difference; the lax truncation (`210, 190`) reduces Run to reverse reads to only 45.7% getting through, where the stricter cut offs let 65.6% through. Hmmmmm. The question is, do we want the strict cut off, but not to be able to pair anything, or do we use the looser cut off, and loose more sequences but be able to pair them?  

In reality, it'll probably be a good idea to use different truncation thresholds for the different runs, based on the average quality scores.

## sample inference: File parsing

At this point, the only thing in the FilteredX folders are the sample 17 files. Let's keep in that way for the moment, and move through the rest of the pipeline with just that sample, to see what it looks like. 

```{r}
# Run1
filtpathF1 <- "./data/run1/16S/filteredF1" # CHANGE ME to the directory containing your filtered forward fastq files
filtpathR1 <- "./data/run1/16S/filteredR1" # CHANGE ME ...
filtFs1 <- list.files(filtpathF1, full.names = TRUE)
filtRs1 <- list.files(filtpathR1, full.names = TRUE)

# Run2
filtpathF2 <- "./data/run2/16S/filteredF2" # CHANGE ME to the directory containing your filtered forward fastq files
filtpathR2 <- "./data/run2/16S/filteredR2" # CHANGE ME ...
filtFs2 <- list.files(filtpathF2, full.names = TRUE)
filtRs2 <- list.files(filtpathR2, full.names = TRUE)
```


```{r}
# Run1
sample.namesF1 <- sapply(strsplit(basename(filtFs1), "_"), `[`, 1) 
# Assumes filename = samplename_XXX.fastq.gz
sample.namesR1 <- sapply(strsplit(basename(filtRs1), "_"), `[`, 1) 
# Assumes filename = samplename_XXX.fastq.gz
if(!identical(sample.namesF1, sample.namesR1)) stop("Forward and reverse files do not match.")
names(filtFs1) <- sample.namesF1
names(filtRs1) <- sample.namesR1

# Run2
sample.namesF2 <- sapply(strsplit(basename(filtFs2), "_"), `[`, 1) 
# Assumes filename = samplename_XXX.fastq.gz
sample.namesR2 <- sapply(strsplit(basename(filtRs2), "_"), `[`, 1) 
# Assumes filename = samplename_XXX.fastq.gz
if(!identical(sample.namesF2, sample.namesR2)) stop("Forward and reverse files do not match.")
names(filtFs2) <- sample.namesF2
names(filtRs2) <- sample.namesR2
```

## learn error rates on subset of data
```{r}
set.seed(100)
```

## Learn error rates

I've set these blocks to not evaluate, since there's only one sample in each of the folders called.

```{r, eval=FALSE}
NSAM.LEARN <- 50 # Choose enough samples to have at least 1M reads
drp.learnF <- derepFastq(sample(filtFs, NSAM.LEARN))
dd.learnF <- dada(drp.learnF, err=NULL, selfConsist=TRUE, multithread=TRUE)
errF <- dd.learnF[[1]]$err_out
write.csv(errF, './data/run1/error-profile-F.csv')
rm(drp.learnF);rm(dd.learnF)
```

```{r, eval=F}
drp.learnR <- derepFastq(sample(filtRs, NSAM.LEARN))
dd.learnR <- dada(drp.learnR, err=NULL, selfConsist=TRUE, multithread=TRUE)
errR <- dd.learnR[[1]]$err_out
write.csv(errR, './data/run1/error-profile-R.csv')
rm(drp.learnR);rm(dd.learnR)
```

Instead, let's look at just our one sample, and set `err=NULL` in the `dada()` function to learn the error rates denovo for each sample. 

```{r}
# dereplicate Run1 and Run2, for and rev
testF1 <- derepFastq(filtFs1[[1]])
testR1 <- derepFastq(filtRs1[[1]])

testF2 <- derepFastq(filtFs2[[1]])
testR2 <- derepFastq(filtRs2[[1]])

# Sample inference from Run1 and Run2, for and rev
ddTestF1 <- dada(testF1, err=NULL, selfConsist=TRUE)
ddTestR1 <- dada(testR1, err=NULL, selfConsist=TRUE)

ddTestF2 <- dada(testF2, err=NULL, selfConsist=TRUE)
ddTestR2 <- dada(testR2, err=NULL, selfConsist=TRUE)

# merge reads and make a sequence table
# run1
mergerTest1 <- mergePairs(ddTestF1, testF1, ddTestR1, testR1, justConcatenate = TRUE)
seqtabTest1 <- makeSequenceTable(mergerTest1)
dim(seqtabTest1)

# run2
mergerTest2 <- mergePairs(ddTestF2, testF2, ddTestR2, testR2, justConcatenate = TRUE)
seqtabTest2 <- makeSequenceTable(mergerTest2)
dim(seqtabTest2)
```

So that gives us 40 RSVs in Run1 and 66 RSVs in Run2. That doesn't seem too bad, honestly. 


# Inspect distribution of sequence lengths

```{r}
table(nchar(getSequences(seqtabTest1)))
hist(nchar(getSequences(seqtabTest1)))

table(nchar(getSequences(seqtabTest2)))
hist(nchar(getSequences(seqtabTest2)))

```

Oh, if we just concatinate the sequences after strict truncation, then of course we're going to end up with sequences that are all exactly the same length. 

Let's look at the top most abundant read, which accounts for most of the sequneces, in each of the runs. If they match, I think we should be good:

``` {r}

seqtabTest2[1,1]

seqtabTest2[1,1]
```

And they match! Next step: try it on all the samples and see if we get good overlap. 

